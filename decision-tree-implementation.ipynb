{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TP5: Decision trees & random forests\nThe aim of this tutorial is to get familiar with the use of decision trees and their generalizations on simple examples using `scikit-learn` tools.","metadata":{}},{"cell_type":"markdown","source":"## Completing your installation first\nYou will need to install packages `python-graphviz` first. If needed, uncomment the `conda` command below:","metadata":{}},{"cell_type":"code","source":"# If needed, uncomment the line below:\n# pip install graphviz\n# import os\n# os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Program Files\\\\Graphviz\\\\bin\\\\'","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pylab import *\n\n# Load matplotlib\nimport matplotlib.pyplot as plt\n\n# Load the library with the iris dataset\nfrom sklearn.datasets import load_iris, load_wine\n\n# Load scikit's decision tree classifier\nfrom sklearn import tree\n\n# Load scikit's random forest classifier library\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n# To visualize trees\nimport graphviz \n\n# Load pandas to manipulate data frames (Excel like)\nimport pandas as pd\n\n# Load seaborn\nimport seaborn as sns\n\n# Set random seed\nnp.random.seed(0)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data for this tutorial is famous. Called, **the iris dataset**, it contains four variables measuring various parts of iris flowers of three related species, and then a fourth variable with the species name. The reason it is so famous in machine learning and statistics communities is because the data requires very little preprocessing (i.e. no missing values, all features are floating numbers, etc.).","metadata":{}},{"cell_type":"code","source":"iris = load_iris()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: explore the data set\n1. What is the structure of the object `iris` ?\n\n2. Plot this dataset in a well chosen set of representations to explore the data.","metadata":{}},{"cell_type":"markdown","source":"## Using `pandas` to manipulate the data\nPandas is great to manipulate data in a Microsoft Excel like way.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create a dataframe with the four feature variables\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# View the top 5 rows\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add a new column with the species names, this is what we are going to try to predict\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n# View the top 5 rows\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Question 1\n\n`iris` is a `sklearn.utils.Bunch` object, which is an extension of a dictionary. It contains the following key/value couples:\n- `data`: a `numpy.array` of `float` containing the different attributes for each sample of the Iris dataset.\n- `feature_names`: a `list` explaining what each value of a sample corresponds to. Here, they correspond to various characteristics of the flower, given in cm.\n- `target`: a `numpy.array` of `int` containing the class label of each sample.\n- `target_names`: a `numpy.array` of `String` explaining what species of flower each class label corresponds to.\n\nThere are also key/value couples containing metadata about the dataset that we won't be using here.","metadata":{}},{"cell_type":"markdown","source":"### Question 2","metadata":{}},{"cell_type":"code","source":"df.species.value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset contains 150 samples, equally distributed between the 3 species (50 of each).\n\nLet's represent this dataset using a `seaborn.pairplot`, which plots pairwise relationships between the features. It allows us to have a visual representation of these attributes even though the dimension of data is greater than 2.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, hue=\"species\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that in this pairwise representation, the `setosa` species seems to always be linearly separable from the other two species. However, there is more overlap between the `versicolor` and `virginica` species, which could make the prediction more challenging for these two classes.\n\nWe can also observe that the separation between the 3 classes is clearer for the `petal width` and `petal length` attributes, which means they will probably be decisive attributes for the classification.\n\nLet's use a last representation to confirm our observations: a `boxplot`, which shows the repartition of data for a single attribute. Let's compare the `boxplots` of one attribute for which the distribution between classes is clear (`petal length`), and one for which it isn't (`sepal length`).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.boxplot(x=\"species\", y=\"petal length (cm)\", data=df)\n\nplt.subplot(122)\nsns.boxplot(x=\"species\", y=\"sepal length (cm)\", data=df)\n\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected, the distribution for `petal length` is very concentrated, even though there is a bit of overlap between `versicolor` and `virginica`. For `sepal length`, the distributions are more spread which causes overlap between the 3 classes.","metadata":{}},{"cell_type":"markdown","source":"## Step 2: create training and test sets","metadata":{}},{"cell_type":"markdown","source":"Create a new column that for each row, generates a random number between 0 and 1, and if that value is less than or equal to .75, then sets the value of that cell as True and false otherwise. This is a quick and dirty way of randomly assigning some rows to be used as the training data and some as the test data.","metadata":{}},{"cell_type":"code","source":"df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n\n# View the top 5 rows\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create two new dataframes, one with the training rows, one with the test rows\ntrain, test = df[df['is_train']==True], df[df['is_train']==False]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the number of observations for the test and training dataframes\nprint('Number of observations in the training data:', len(train))\nprint('Number of observations in the test data:',len(test))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a list of the feature column's names\nfeatures = df.columns[:4]\n\n# View features\nfeatures","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train['species'] contains the actual species names. Before we can use it,\n# we need to convert each species name into a digit. So, in this case there\n# are three species, which have been coded as 0, 1, or 2.\ny = pd.factorize(train['species'])[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: decision trees for the iris dataset\nThe method `tree.DecisionTreeClassifier()` from `scikit-learn` builds decision trees objects as follows:","metadata":{}},{"cell_type":"code","source":"clf = tree.DecisionTreeClassifier()\nclf = clf.fit(train[features], y)\n\n# Using the whole dataset you may use directly:\n#clf = clf.fit(iris.data, iris.target)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `export_graphviz` exporter supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:","metadata":{}},{"cell_type":"code","source":"dot_data = tree.export_graphviz(clf, out_file=None, \n                         feature_names=iris.feature_names,  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also export the tree in Graphviz format and  savethe resulting graph in an output file iris.pdf:","metadata":{}},{"cell_type":"code","source":"dot_data = tree.export_graphviz(clf, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"iris\") ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After being fitted, **the model can then be used to predict the class of samples**:","metadata":{}},{"cell_type":"code","source":"class_pred = clf.predict(iris.data[:1, :])\nclass_pred","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 1\n1. Train the decision tree on the iris dataset and explain how one should read blocks in `graphviz` representation of the tree.\n\n2. Plot the regions of decision with the points of the training set superimposed.\n\n*Indication: you may find the function `plt.contourf` useful.","metadata":{}},{"cell_type":"markdown","source":"### Question 1\n\nThe decision tree was trained in the above cells. The blocks in `graphviz` give information about every node, including:\n- the attribute used to partition the node as well as the threshold used to classify samples;\n- the gini index, which is a measure of impurity: the closest it is to 0, the purest the node;\n- the number of samples in this part of the tree (reachable from this node);\n- the repartition of these samples in the 3 classes;\n- the dominant class in this node.","metadata":{}},{"cell_type":"markdown","source":"### Question 2","metadata":{}},{"cell_type":"markdown","source":"The code to display pairwise regions of decision was found on the [scikit documentation website](https://scikit-learn.org/0.15/auto_examples/tree/plot_iris.html). We slightly changed the way predictions are displayed for clarity's sake.","metadata":{}},{"cell_type":"code","source":"# Parameters\nn_classes = 3\nplot_colors = \"brg\"\nplot_step = 0.02\n\nplt.figure(figsize=(20,10))\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n                                [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Shuffle\n    idx = np.arange(X.shape[0])\n    np.random.seed(13)\n    np.random.shuffle(idx)\n    X = X[idx]\n    y = y[idx]\n\n    # Standardize\n    mean = X.mean(axis=0)\n    std = X.std(axis=0)\n    X = (X - mean) / std\n\n    # Train\n    clf_display = tree.DecisionTreeClassifier().fit(X, y)\n\n    # Plot the decision boundary\n    plt.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf_display.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, colors=['b', 'y', 'g', 'r', 'purple'], alpha = 0.3)\n\n    plt.xlabel(iris.feature_names[pair[0]])\n    plt.ylabel(iris.feature_names[pair[1]])\n    plt.axis(\"tight\")\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c = color, label=iris.target_names[i],\n                    cmap=plt.cm.Pastel1)\n\n    plt.axis(\"tight\")\n\nplt.suptitle(\"Decision surface of a decision tree using paired features\")\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we expected, some pairs of attribute are more easily separable than others, especially those involving the `petal length` and `petal width` features. \n\nWe also confirmed that `setosa` is more easily predicted than `versicolor` and `virginica`. However, even in the regions where there is a lot of overlap between them, the tree is able to separate them fairly well.\n\nWe can also notice that there may be some overfitting, since some regions contain single isolated points of the training set. This is a known weakness of decision trees and will be mitigated by using random forests.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 2\n1. Build 2 different trees based on a sepal features (sepal lengths, sepal widths) vs petal features (petal lengths, petal widths) only: which features are the most discriminant?\n\n2. Compare performances with those obtained using all features.\n\n3. Try the same as above using the various splitting criterion available, Gini's index, classification error or cross-entropy. Comment on your results. ","metadata":{}},{"cell_type":"markdown","source":"### Question 1","metadata":{}},{"cell_type":"code","source":"## Tree based on sepal features\nfeatures_sepal = features[0:2]\ny = pd.factorize(train['species'])[0]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(train[features_sepal], y)\n\ndot_data = tree.export_graphviz(clf, out_file=None, \n                         feature_names=iris.feature_names[0:2],  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Tree based on petal features\nfeatures_petal = features[2:4]\ny = pd.factorize(train['species'])[0]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(train[features_petal], y)\n\ndot_data = tree.export_graphviz(clf, out_file=None, \n                         feature_names=iris.feature_names[2:4],  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we could already tell from the regions of decision, the petal features seem to be much more discriminant than the sepal features. Indeed, the tree generated from sepal features is much more complex and deep than the one generated with petal features: it shows that each node struggles to distinguish classes based on sepal features only.","metadata":{}},{"cell_type":"markdown","source":"### Question 2","metadata":{}},{"cell_type":"markdown","source":"To compare the performances between the trees, we will compare the scores and confusion matrices they obtain on the test set.","metadata":{}},{"cell_type":"code","source":"# Training\ny = pd.factorize(train['species'])[0]\ny_test = pd.factorize(test['species'])[0]\n\nclf_full = tree.DecisionTreeClassifier()\nclf_full = clf_full.fit(train[features], y)\n\nclf_petal= tree.DecisionTreeClassifier()\nclf_petal = clf_petal.fit(train[features_petal], y)\n\nclf_sepal = tree.DecisionTreeClassifier()\nclf_sepal = clf_sepal.fit(train[features_sepal], y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Scores\npred_full = clf_full.predict(test[features])\nscore_full = accuracy_score(y_test, pred_full)\nprint(f\"The score obtained with all features is: {score_full}\")\n\npred_petal = clf_petal.predict(test[features_petal])\nscore_petal = accuracy_score(y_test, pred_petal)\nprint(f\"The score obtained with petal features only is: {score_petal}\")\n\npred_sepal = clf_sepal.predict(test[features_sepal])\nscore_sepal= accuracy_score(y_test, pred_sepal)\nprint(f\"The score obtained with sepal features only is: {score_sepal}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrices\npd.crosstab(test['species'], pred_full, rownames=['Actual Species'], colnames=['Predicted Species (all features)'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.crosstab(test['species'], pred_petal, rownames=['Actual Species'], colnames=['Predicted Species (petal features)'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.crosstab(test['species'], pred_sepal, rownames=['Actual Species'], colnames=['Predicted Species (sepal features)'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that:\n- sepal features give a very poor score, which was to be expected;\n- using petal features give an even better score than using all features: it shows that not only are sepal features not very discriminant, they even create more confusion for the classifier, which lowers the score;\n- the errors mostly occur with confusion between `versicolor` and `virginica`, further illustrating previous observations.","metadata":{}},{"cell_type":"markdown","source":"### Question 3\n\nLet's compare the 3 scores we just obtained with Gini index to the results with cross entropy.","metadata":{}},{"cell_type":"code","source":"# Training with \ny = pd.factorize(train['species'])[0]\ny_test = pd.factorize(test['species'])[0]\n\nclf_full = tree.DecisionTreeClassifier(criterion=\"entropy\")\nclf_full = clf_full.fit(train[features], y)\n\nclf_petal= tree.DecisionTreeClassifier(criterion=\"entropy\")\nclf_petal = clf_petal.fit(train[features_petal], y)\n\nclf_sepal = tree.DecisionTreeClassifier(criterion=\"entropy\")\nclf_sepal = clf_sepal.fit(train[features_sepal], y)\n\n# Scores\npred_full = clf_full.predict(test[features])\nscore_full = accuracy_score(y_test, pred_full)\nprint(f\"The score obtained with all features is: {score_full}\")\n\npred_petal = clf_petal.predict(test[features_petal])\nscore_petal = accuracy_score(y_test, pred_petal)\nprint(f\"The score obtained with petal features only is: {score_petal}\")\n\npred_sepal = clf_sepal.predict(test[features_sepal])\nscore_sepal= accuracy_score(y_test, pred_sepal)\nprint(f\"The score obtained with sepal features only is: {score_sepal}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that the accuracy scores are better using cross entropy. In class we saw that the choice of a criterion is an empirical process, since it depends on the problem at hand. ","metadata":{}},{"cell_type":"markdown","source":"### Going further ahead (not mandatory) \nTry the same approach adapted to another toy dataset from `scikit-learn` described at:\nhttp://scikit-learn.org/stable/datasets/index.html\n\nPlay with another dataset available at:\nhttp://archive.ics.uci.edu/ml/datasets.html","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Random forests\nGo to \n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html \n\nfor a documentation about the `RandomForestClassifier` provided by `scikit-learn`.\n\nSince target values must be integers, we first need to transform labels into numbers as below.","metadata":{}},{"cell_type":"code","source":"# train['species'] contains the actual species names. Before we can use it,\n# we need to convert each species name into a digit. So, in this case there\n# are three species, which have been coded as 0, 1, or 2.\ny = pd.factorize(train['species'])[0]\n\n# View target\ny","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a random forest Classifier. By convention, clf means 'Classifier'\nrf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier to take the training features and learn how they relate\n# to the training y (the species)\nrf.fit(train[features], y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Make predictions** and create actual english names for the plants for each predicted plant class:","metadata":{}},{"cell_type":"code","source":"preds = rf.predict(test[features])\npreds_names = pd.Categorical.from_codes(preds, iris.target_names)\npreds_names","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create a confusion matrix","metadata":{}},{"cell_type":"code","source":"# Create confusion matrix unsing pandas:\npd.crosstab(test['species'], preds, rownames=['Actual Species'], colnames=['Predicted Species'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature selection using random forests byproducts\n\nOne of the interesting use cases for random forest is feature selection. One of the byproducts of trying lots of decision tree variations is that you can examine which variables are working best/worst in each tree.\n\nWhen a certain tree uses one variable and another doesn't, you can compare the value lost or gained from the inclusion/exclusion of that variable. The good random forest implementations are going to do that for you, so all you need to do is know which method or variable to look at.","metadata":{}},{"cell_type":"markdown","source":"### View feature importance\nWhile we don't get regression coefficients like with ordinary least squares (OLS), we do get a score telling us how important each feature was in classifying. This is one of the most powerful parts of random forests, because we can clearly see that petal width was more important in classification than sepal width.","metadata":{}},{"cell_type":"code","source":"# View a list of the features and their importance scores\nlist(zip(train[features], rf.feature_importances_))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 3\n1. Comment on the feature importances with respect to your previous observations on decision trees above.\n\n2. Extract and visualize 5 trees belonging to the random forest using the attribute `estimators_` of the trained random forest classifier. Compare them. *Note that you may code a loop on extracted trees.*\n\n3. Study the influence of parameters like `max_depth`, `min_samples_leaf` and `min_samples_split`. Try to optimize them and explain your approach and choices.\n\n4. How is estimated the prediction error of a random forest ?\n*Indication: have a look at parameter `oob_score`.*\nWhat are out-of-bag samples ?\n\n5. What should you do when classes are not balanced in the dataset ? (that is when there are much more examples of one class than another)","metadata":{}},{"cell_type":"markdown","source":"### Question 1\n\nThe feature importances confirm the observations we had already made at previous questions: petal features are way more significant than sepal features.","metadata":{}},{"cell_type":"markdown","source":"### Question 2","metadata":{}},{"cell_type":"code","source":"for estimator in rf.estimators_[:5] :\n    dot_data = tree.export_graphviz(estimator, out_file=None, \n                         feature_names=iris.feature_names,  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \n    graph = graphviz.Source(dot_data)  \n    # graph.view() # uncomment to display the 5 graphs in browser","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that the trees have very different structures (chosen features, thresholds...): it illustrates that the decision tree method is very unstable, hence the motivation to use a random forest and average results over the different estimators.\n\nWe can also notice that the trees tend to be longer when the first node uses sepal features to split the dataset, which further confirms their poor separating power.","metadata":{}},{"cell_type":"markdown","source":"### Question 3","metadata":{}},{"cell_type":"code","source":"y = pd.factorize(train['species'])[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Choosing max_depth\nbest_max_depth = 0\nbest_score = 0\nscores = []\nfor max_depth in range(1,20):\n    rf = RandomForestClassifier(n_jobs=2, random_state=0, max_depth=max_depth, oob_score=True)\n    rf.fit(train[features], y)\n    pred = rf.predict(test[features])\n    score = rf.oob_score_\n    scores.append(score)\n    if score > best_score :\n        best_score = score\n        best_max_depth = max_depth\nprint(f\"Best oob score: {best_score} | Best max_depth: {best_max_depth}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(scores)\nplt.title(\"Evolution of score depending of max_depth\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choosing min_samples_leaf\nbest_min_samples_leaf = 0\nbest_score = 0\nscores = []\nfor min_samples_leaf in range(1,20):\n    rf = RandomForestClassifier(n_jobs=2, random_state=0, min_samples_leaf=min_samples_leaf, oob_score=True)\n    rf.fit(train[features], y)\n    pred = rf.predict(test[features])\n    score = rf.oob_score_\n    scores.append(score)\n    if score > best_score :\n        best_score = score\n        best_min_samples_leaf = min_samples_leaf\nprint(f\"Best score: {best_score} | Best min_samples_leaf: {best_min_samples_leaf}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(scores)\nplt.title(\"Evolution of score depending of min_samples_leaf\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choosing min_samples_split\nbest_min_samples_split = 0\nbest_score = 0\nscores = []\nfor min_samples_split in range(2,20):\n    rf = RandomForestClassifier(n_jobs=2, random_state=0, min_samples_split=min_samples_split, oob_score=True)\n    rf.fit(train[features], y)\n    pred = rf.predict(test[features])\n    score = score = rf.oob_score_\n    scores.append(score)\n    if score > best_score :\n        best_score = score\n        best_min_samples_split = min_samples_split\nprint(f\"Best score: {best_score} | Best min_samples_split: {best_min_samples_split}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(scores)\nplt.title(\"Evolution of score depending of min_samples_leaf\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trying to train a model with all 3 best parameters\nrf = RandomForestClassifier(n_jobs=2, random_state=0, min_samples_split=best_min_samples_split, max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf)\nrf.fit(train[features], y)\npred = rf.predict(test[features])\nscore = accuracy_score(y_test, pred)\nprint(f\"Accuracy score with best parameters: {score}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for estimator in rf.estimators_[:5] :\n    dot_data = tree.export_graphviz(estimator, out_file=None, \n                         feature_names=iris.feature_names,  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \n    graph = graphviz.Source(dot_data)  \n    # graph.view() # uncomment to display the 5 graphs in browser","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To choose the best parameters, we tried to tune them individually by looping over values between 1 and 20 (which seem to be reasonable values given the trees we had obtained earlier). We then chose the 3 values that gave the best results as our optimized set of parameters.\n\nWe used the `oob_score`  to evaluate the performance of the different estimator (cf next questions for explanations).\n\nHere's what we can observe:\n- The 3 parameters, though they do influence the `oob_score`, don't have that much of an impact on it (a few percents at most).\n- The parameters don't seem to follow a given law, hence the need to determine them empirically.\n- The `oob_score` obtained with the 3 best parameters is better than each of the individual best scores, which shows that our method seems to be efficient.","metadata":{}},{"cell_type":"markdown","source":"### Question 4\n\nThe prediction error of a random forest is estimated by the mistakes it makes on out-of-bag samples, that's to say the samples that were not used during the training process because of bootstrapping. It evaluates the ability of the estimator to generalize on data it had not \"seen\" during training.","metadata":{}},{"cell_type":"markdown","source":"### Question 5\n\nA solution to mitigate imbalance would be to over-sample the minority class or under-sample the majority class during bootstrapping, so that the resulting dataset becomes balanced. Another solution would be to use class weighting, which means placing a heavier penalty on misclassifying the minority class. \n\nBoth solutions can be implemented using `sklearn` or `imblearn`, as explained in more detail in [this article](https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/). ","metadata":{}},{"cell_type":"markdown","source":"## Step 5: a small example of regression using random forests\nRandom forest is capable of learning without carefully crafted data transformations. Take the the $f(x) = \\sin(x)$ function for example.\n\nCreate some fake data and add a little noise.","metadata":{}},{"cell_type":"code","source":"x = np.random.uniform(-2.5, 2.5, 1000)\ny = np.sin(x) + np.random.normal(0, .1, 1000)\n\nplt.plot(x,y,'ko',markersize=1,label='data')\nplt.plot(np.arange(-2.5,2.5,0.1),np.sin(np.arange(-2.5,2.5,0.1)),'r-',label='ref')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we try and build a basic linear model to predict y using x we end up with a straight line that sort of bisects the sin(x) function. Whereas if we use a random forest, it does a much better job of approximating the sin(x) curve and we get something that looks much more like the true function.\n\nBased on this example, we will illustrate how the random forest isn't bound by linear constraints.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 4\n1. Apply random forests on this dataset for regression and compare performances with ordinary least squares regression.\n*Note that ordinay least square regression is available thanks to:\nfrom sklearn.linear_model import LinearRegression*\n\n2. Comment on your results.","metadata":{}},{"cell_type":"markdown","source":"### Question 1","metadata":{}},{"cell_type":"markdown","source":"### Indications:\nYou may use half of points for training and others to test predictions. Then you will have an idea of how far the random forest predictor fits the sinus curve.\n\nTo this aim, you will need to use the model `RandomForestRegressor`. Be careful that when only 1 feature `x` is used as an input, you will need to reshape it by `x.reshape(-1,1)` when using methods `fit` and `predict`.","metadata":{}},{"cell_type":"code","source":"models = [RandomForestRegressor(n_estimators=30, max_depth=4),\n          LinearRegression()]\n\nplt.figure(figsize=(15,5))\n\nfor i, model in enumerate(models):\n    # Training regressor\n    model.fit(x[0::2].reshape(-1, 1),y[0::2])\n    \n    # Testing the regressor\n    pred = model.predict(x[1::2].reshape(-1, 1))\n    plt.subplot(1,len(models), i+1)\n    plt.title(f\"Regression with {type(model).__name__}\")\n    plt.plot(x[1::2],pred,'bo',markersize=1,label='data')\n    plt.plot(np.arange(-2.5,2.5,0.1),np.sin(np.arange(-2.5,2.5,0.1)),'r-',label='ref')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Question 2\n\nAs expected, the linear regression is only able to learn a linear function and is therefore unable to predict the `sin` curve. The `RandomForestRegressor`, on the other hand, does a good job at approximating the curve, since it isn't bound by a specific model of functions.\n\nOut of curiosity and to understand how it works, we display one of the trees of the `RandomForestRegressor`.","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=30, max_depth=4)\nmodel.fit(x[0::2].reshape(-1, 1),y[0::2])\n\nestimator = model.estimators_[0]\ndot_data = tree.export_graphviz(estimator, out_file=None,  \n                        filled=True, rounded=True,  \n                        special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We note that the function used to classify points is a squared error function. The strength of random forests here is that by averaging the results given by 100 trees, it is able to reconstruct a complex function quite well.","metadata":{}},{"cell_type":"markdown","source":"## Documentation\n\n### Decision trees\nhttp://scikit-learn.org/stable/modules/tree.html\n\n### Random forests\nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\n### Plot decision surface : using `plt.contourf`\nhttp://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py","metadata":{}},{"cell_type":"markdown","source":"## Pruning trees: not available in scikit-learn.\nSince post-pruning of tree is not implemented in scikit-learn, you may think of coding your own pruning function. For instance, taking into account the numer of samples per leaf as proposed below:","metadata":{}},{"cell_type":"code","source":"# Pruning function (useful ?)\ndef prune(decisiontree, min_samples_leaf = 1):\n    if decisiontree.min_samples_leaf >= min_samples_leaf:\n        raise Exception('Tree already more pruned')\n    else:\n        decisiontree.min_samples_leaf = min_samples_leaf\n        tree = decisiontree.tree_\n        for i in range(tree.node_count):\n            n_samples = tree.n_node_samples[i]\n            if n_samples <= min_samples_leaf:\n                tree.children_left[i]=-1\n                tree.children_right[i]=-1\n                ","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}